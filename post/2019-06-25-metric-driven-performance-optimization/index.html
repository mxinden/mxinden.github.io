<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Metric driven performance optimization | Max Inden</title>
<meta name="keywords" content="tech, prometheus, monitoring, kubernetes">
<meta name="description" content="Within my work at Red Hat and Kubernetes SIG instrumentation I have been working on kube-state-metrics , a Prometheus exporter exposing the state of a Kubernetes cluster to a Prometheus monitoring system. In particular I have focused on performance optimizing metric rendering for both latency as well as resource usage. Below I want to describe our approach of metric driven performance tuning, using Prometheus to monitor kube-state-metrics on top of Kubernetes, which in itself enables Prometheus to monitor Kubernetes.">
<meta name="author" content="Max Inden">
<link rel="canonical" href="https://max-inden.de/post/2019-06-25-metric-driven-performance-optimization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://max-inden.de/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://max-inden.de/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://max-inden.de/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://max-inden.de/apple-touch-icon.png">
<link rel="mask-icon" href="https://max-inden.de/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://max-inden.de/post/2019-06-25-metric-driven-performance-optimization/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Metric driven performance optimization" />
<meta property="og:description" content="Within my work at Red Hat and Kubernetes SIG instrumentation I have been working on kube-state-metrics , a Prometheus exporter exposing the state of a Kubernetes cluster to a Prometheus monitoring system. In particular I have focused on performance optimizing metric rendering for both latency as well as resource usage. Below I want to describe our approach of metric driven performance tuning, using Prometheus to monitor kube-state-metrics on top of Kubernetes, which in itself enables Prometheus to monitor Kubernetes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://max-inden.de/post/2019-06-25-metric-driven-performance-optimization/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2019-06-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-06-24T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Metric driven performance optimization"/>
<meta name="twitter:description" content="Within my work at Red Hat and Kubernetes SIG instrumentation I have been working on kube-state-metrics , a Prometheus exporter exposing the state of a Kubernetes cluster to a Prometheus monitoring system. In particular I have focused on performance optimizing metric rendering for both latency as well as resource usage. Below I want to describe our approach of metric driven performance tuning, using Prometheus to monitor kube-state-metrics on top of Kubernetes, which in itself enables Prometheus to monitor Kubernetes."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://max-inden.de/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Metric driven performance optimization",
      "item": "https://max-inden.de/post/2019-06-25-metric-driven-performance-optimization/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Metric driven performance optimization",
  "name": "Metric driven performance optimization",
  "description": "Within my work at Red Hat and Kubernetes SIG instrumentation I have been working on kube-state-metrics , a Prometheus exporter exposing the state of a Kubernetes cluster to a Prometheus monitoring system. In particular I have focused on performance optimizing metric rendering for both latency as well as resource usage. Below I want to describe our approach of metric driven performance tuning, using Prometheus to monitor kube-state-metrics on top of Kubernetes, which in itself enables Prometheus to monitor Kubernetes.",
  "keywords": [
    "tech", "prometheus", "monitoring", "kubernetes"
  ],
  "articleBody": "Within my work at Red Hat and Kubernetes SIG instrumentation I have been working on kube-state-metrics , a Prometheus exporter exposing the state of a Kubernetes cluster to a Prometheus monitoring system. In particular I have focused on performance optimizing metric rendering for both latency as well as resource usage. Below I want to describe our approach of metric driven performance tuning, using Prometheus to monitor kube-state-metrics on top of Kubernetes, which in itself enables Prometheus to monitor Kubernetes.\nWhat is kube-state-metrics Kube-state-metrics is a Prometheus exporter exposing metrics about the state of Kubernetes objects such as Pods and Deployments to Prometheus. It listens for state changes via the Kubernetes API and exposes this state in the Prometheus format on http://xxx/metrics on each scrape by Prometheus.\n``` +-----------+ +-------------------+ +-------------+ | APIServer | | kubestatemetrics | | Prometheus | +-----------+ +-------------------+ +-------------+ | | | | New Pod p1 | | |---------------------\u003e| | | -----------\\ | | | | Cache p1 |-| | | |----------| | | | | | | | | | | | | | Get all metrics | | |\u003c---------------------------------------| | | --------------------------------\\ | | |-| Render all Kubernetes objects | | | | | into metrics | | | | |-------------------------------| | | | | | | Metrics | | |---------------------------------------\u003e| | | | ``` One can think of kube-state-metrics as an adapter converting Kubernetes Objects\n- apiVersion: v1 kind: Pod metadata: labels: app: kube-state-metrics pod-template-hash: 5fc64f676f name: kube-state-metrics-5fc64f676f-gl6v6 namespace: monitoring into Prometheus metrics.\nkube_pod_container_info{container=\"kube-state-metrics\",namespace=\"monitoring\",pod=\"kube-state-metrics-5fc64f676f-gl6v6\"} 1 kube_pod_labels{label_app=\"kube-state-metrics\",label_pod_template_hash=\"5fc64f676f\",namespace=\"monitoring\",pod=\"kube-state-metrics-5fc64f676f-gl6v6\"} 1 Problem with kube-state-metrics \u003c 1.4 Kube-state-metrics v1.4 and below is leveraging Kubernetes client-go and Prometheus client_golang to inter-operate with Kubernetes and Prometheus. Thereby kube-state-metrics only contains little amount of glue code and mostly business logic itself. On the one hand this keeps the complexity of the project low. On the other hand, both Kubernetes client-go as well as Prometheus client_golang are not optimized for the kube-state-metrics use case see e.g. user reports #257 and #493 .\nThe above trade-off serves us well on smaller Kubernetes clusters. Once kube-state-metrics is used on larger clusters with more than 50 MB of metric output, three properties become problematic, consequently forming our optimization targets:\n1. Scrape duration The reoccurring (e.g. every minute) process of Prometheus requesting Kubernetes metrics from kube-state-metrics is called a scrape. Prometheus tracks the start of the process, initializes the http request to kube-state-metrics /metrics, parses the response and saves the values with the start timestamp in its time series database. Both if kube-state-metrics respondes within a second and within a minute, the values are saved with the same timestamp within the Prometheus database.\nOnce scrape durations differ within consecutive scrapes of the same exporter (e.g. kube-state-metrics) or across exporters (kube-state-metrics and cadvisor ), it is difficult to correlate the collected data, as it could be off by as much as the scraping process lasted (minus network latency, http, tcp, ip overhead, …). Thus it is disirable to keep the scrape duration to a minimum. In addition Prometheus even times out by default once a scrape duration reaches 10s.\nThe metric scrape_duration_seconds exposed by Prometheus itself gives us visibility for the above, hence we can use it for the planned optimization.\n2. CPU usage Running kube-state-metrics on large Kubernetes clusters results in high CPU usage of kube-state-metrics as it has to convert the Kubernetes API objects to Prometheus metrics ad-hoc on each scrape of Prometheus. In addition this CPU load is not a flat line, but spiky, given that this CPU intensive process is part of the hot-path of Prometheus scraping the exporter.\nAs with the scrape_duration_seconds metric for the first optimization target, we can leverage the fact that kube-state-metrics can run on a Kubernetes cluster and use the container_cpu_usage_seconds exposed by cadvisor to get visibility into optimizing cpu usage.\n3. Memory usage As mentioned above, kube-state-metrics uses the Kubernetes client-go library to inter-operate with Kubernetes. More specifically it leverages client-go’s informers. While using informers keeps kube-state-metric’s code complexity low, it forces high memory usage upon it, given that kube-state-metrics is interested in all Kubernetes objects of a cluster, thus keeping a copy of all Kubernetes objects within the cache of the infomers. This is not problematic for small Kubernetes clusters but results in large memory allocations on larger ones.\nAnalogous to container_cpu_usage_seconds we can use container_memory_usage_bytes to track the memory usage of kube-state-metrics.\nIntroducing a custom cache As previously mentioned kube-state-metrics v1.4 and below replicates all Kubernetes objects into its internal cache to ad-hoc convert them into the Prometheus metric format, once a new scrape request comes in.\n``` +-----------+ +-------------------+ +-------------+ | APIServer | | kubestatemetrics | | Prometheus | +-----------+ +-------------------+ +-------------+ | | | | New Pod p1 | | |---------------------\u003e| | | -----------\\ | | | | Cache p1 |-| | | |----------| | | | | | | | | | | | | | Get all metrics | | |\u003c---------------------------------------| | | --------------------------------\\ | | |-| Render all Kubernetes objects | | | | | into metrics | | | | |-------------------------------| | | | | | | Metrics | | |---------------------------------------\u003e| | | | This lead us to an idea. Instead of caching the Kubernetes objects themselves, how about rendering them into Prometheus metrics right away and only cache the rendered metrics instead. This has three advantages: First Kubernetes object to metric rendering is not happening in the hot-path (scrape) anymore reducing scrape duration. Second caching metrics instead of full Kubernetes objects reduces needed memory. Third, under the premise that most Kubernetes objects stay untouched between consecutive scrapes, metrics are rendered once per Kubernetes object update, not once per scrape, reducing CPU usage. +———–+ +——————-+ +————-+ | APIServer | | kubestatemetrics | | Prometheus | +———–+ +——————-+ +————-+ | | | | New Pod p1 | | |————————–\u003e| | | ———————–\\ | | | | Render metrics of p1 |-| | | |———————-| | | |————————-\\ | | || Cache rendered metrics |-| | ||————————| | | | | | | | | | | | | | Get all metrics | | |\u003c———————–| | | | | | Metrics | | |———————–\u003e| | | |\nInstead of replacing all of Kubernetes client-go with our own specialized implementation, which increases the maintenance burdon given that Kubernetes is a fast moving project, it is possible to hook into client-go via the [ *Reflector* ](https://github.com/kubernetes/client-go/blob/master/tools/cache/reflector.go) abstraction and implement a custom cache, in our case for metrics instead of Kubernetes objects. The result can be seen in the graph below, showing the duration of Prometheus scrapes of the old version (v1.4) in red and the optimized version in cyan. Introducing the optimized cache lead to a ~10x improvement in our test environment. Please keep in mind that this was under artificial load. Numbers on real world clusters will be shown further below. ## Compression on or off? Next up we looked into compression of the metric payload going back to Prometheus. By default kube-state-metrics v1.4, more precisely Prometheus client_golang, compressed its responses. Doing so sounds reasonable, given that the Prometheus metric format is repetitive, thus great for compression, but still we wanted to investigate this default option, given that most deployments are running in a local area network. We deployed kube-state-metrics in four versions: v1.4, cache-optimized without compression, cache-optimized with golang gzip, cache-optimized with [New York Times gzip](https://github.com/NYTimes/gziphandler). First off we can compare the CPU usage. Without surprise compressing is more CPU intense: Next we can take a look at the memory usage (not allocation). One would think that using compression kube-state-metrics would use less memory. Problem is, that we are not caching a big blob of metrics of all Kubernetes objects, but instead we are caching multiple blobs of metrics per Kubernetes object. That way, once an update for a single Kubernetes object comes in, one does not have to recompute the big blob, but only the metric blob for that specific Kubernetes object. With this in mind, for a scrape we have to cumulate all the blobs and then compress. Thus compression does not decrease the memory footprint, but quite the opposite, increases it. Last we should take a look at the impact of compression on the scrape duration. In our test environment Prometheus and kube-state-metrics are within the same local area network. The graph below shows, that transfering bigger metric blobs in plain-text is faster than transfering smaller compressed metric blobs + compressing and decompressing. Thus, as you can see in the graph below kube-state-metrics with optimized caching but disabled compression has the lowest scrape duration. After testing this in multiple real world scenarios, we ended up adding the option to compress to the new version of kube-state-metrics but disabling it by default. ## Golangs _strings.Builder_ While optimizing kube-state-metrics itself, a lot happened on the Prometheus upstream [common library](https://github.com/prometheus/common/pull/148) in parallel. Among other things the library switched to using Golangs [ strings.Builder ](https://golang.org/pkg/strings/#Builder). Instead of formatting strings via `fmt`, which bears many memory allocations with it, `strings.Builder` uses a byte slice under the hood, leveraging the fact that byte slices in constrary to strings are mutable. Many of these optimizations also landed downstream in kube-state-metrics. ## Benchmarks Given the wide spread usage of kube-state-metrics in the Kubernetes community, many users helped us to test the performance optimized version of kube-state-metrics. You can find multiple benchmarks in [ this ](https://github.com/kubernetes/kube-state-metrics/issues/498) Github thread. --- This blog post is based on a [talk I gave at KubeCon Barcelona 2019](https://kccnceu19.sched.com/event/MPjo/deep-dive-kubernetes-instrumentation-sig-frederic-branczyk-max-inden-red-hat). The [recording](https://www.youtube.com/watch?v=dvk_-NCK1Ls) is online. The [slides can be downloaded here](/static/metric-driven-performance-optimization/slides.pdf). ",
  "wordCount" : "1597",
  "inLanguage": "en",
  "datePublished": "2019-06-24T00:00:00Z",
  "dateModified": "2019-06-24T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Max Inden"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://max-inden.de/post/2019-06-25-metric-driven-performance-optimization/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Max Inden",
    "logo": {
      "@type": "ImageObject",
      "url": "https://max-inden.de/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://max-inden.de/" accesskey="h" title="Max Inden (Alt + H)">Max Inden</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://max-inden.de/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://max-inden.de/resume" title="Resume">
                    <span>Resume</span>
                </a>
            </li>
            <li>
                <a href="https://max-inden.de/readings" title="Readings">
                    <span>Readings</span>
                </a>
            </li>
            <li>
                <a href="https://max-inden.de/post/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://max-inden.de/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://max-inden.de/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Metric driven performance optimization
    </h1>
    <div class="post-meta"><span title='2019-06-24 00:00:00 +0000 UTC'>June 24, 2019</span>&nbsp;·&nbsp;Max Inden

</div>
  </header> 
  <div class="post-content"><p>Within my work at Red Hat and Kubernetes <a href="https://github.com/kubernetes/community/tree/master/sig-instrumentation"> SIG instrumentation
</a> I
have been working on <a href="https://github.com/kubernetes/kube-state-metrics/"> kube-state-metrics
</a>, a <a href="https://github.com/prometheus/prometheus"> Prometheus
</a> exporter exposing the state of a
Kubernetes cluster to a Prometheus monitoring system. In particular I have
focused on performance optimizing metric rendering for both latency as well as
resource usage. Below I want to describe our approach of metric driven
performance tuning, using Prometheus to monitor kube-state-metrics on top of
Kubernetes, which in itself enables Prometheus to monitor Kubernetes.</p>
<h2 id="what-is-kube-state-metrics">What is kube-state-metrics<a hidden class="anchor" aria-hidden="true" href="#what-is-kube-state-metrics">#</a></h2>
<p>Kube-state-metrics is a <a href="https://prometheus.io/docs/instrumenting/exporters/"> Prometheus exporter
</a> exposing metrics about
the state of Kubernetes objects such as <em>Pods</em> and <em>Deployments</em> to Prometheus.
It listens for state changes via the Kubernetes API and exposes this state in
the <a href="https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-format-example"> Prometheus format
</a>
on <code>http://xxx/metrics</code> on each scrape by Prometheus.</p>
<small>
```
+-----------+      +-------------------+                       +-------------+
| APIServer |      | kubestatemetrics  |                       | Prometheus  |
+-----------+      +-------------------+                       +-------------+
      |                      |                                        |
      | New Pod p1           |                                        |
      |--------------------->|                                        |
      |         -----------\ |                                        |
      |         | Cache p1 |-|                                        |
      |         |----------| |                                        |
      |                      |                                        |
      |                      |                                        |
      |                      |                                        |
      |                      |                        Get all metrics |
      |                      |<---------------------------------------|
      |                      | --------------------------------\      |
      |                      |-| Render all Kubernetes objects |      |
      |                      | | into metrics                  |      |
      |                      | |-------------------------------|      |
      |                      |                                        |
      |                      | Metrics                                |
      |                      |--------------------------------------->|
      |                      |                                        |
```
</small>
<p>One can think of kube-state-metrics as an adapter converting Kubernetes Objects</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>- <span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">kube-state-metrics</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">pod-template-hash</span>: <span style="color:#ae81ff">5fc64f676f</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">kube-state-metrics-5fc64f676f-gl6v6</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">monitoring</span>
</span></span></code></pre></div><p>into Prometheus metrics.</p>
<pre tabindex="0"><code>kube_pod_container_info{container=&#34;kube-state-metrics&#34;,namespace=&#34;monitoring&#34;,pod=&#34;kube-state-metrics-5fc64f676f-gl6v6&#34;} 1        
kube_pod_labels{label_app=&#34;kube-state-metrics&#34;,label_pod_template_hash=&#34;5fc64f676f&#34;,namespace=&#34;monitoring&#34;,pod=&#34;kube-state-metrics-5fc64f676f-gl6v6&#34;} 1
</code></pre><h2 id="problem-with-kube-state-metrics--14">Problem with kube-state-metrics &lt; 1.4<a hidden class="anchor" aria-hidden="true" href="#problem-with-kube-state-metrics--14">#</a></h2>
<p>Kube-state-metrics v1.4 and below is leveraging Kubernetes <a href="https://github.com/kubernetes/client-go"> client-go
</a> and Prometheus <a href="https://github.com/prometheus/client_golang/"> client_golang
</a> to inter-operate with Kubernetes
and Prometheus. Thereby kube-state-metrics only contains little amount of glue
code and mostly business logic itself. On the one hand this keeps the complexity
of the project low. On the other hand, both Kubernetes client-go as well as
Prometheus client_golang are not optimized for the kube-state-metrics use case
see e.g. user reports <a href="https://github.com/kubernetes/kube-state-metrics/issues/257"> #257
</a> and <a href="https://github.com/kubernetes/kube-state-metrics/issues/257"> #493
</a>.</p>
<p>The above trade-off serves us well on smaller Kubernetes clusters. Once
kube-state-metrics is used on larger clusters with more than 50 MB of metric
output, three properties become problematic, consequently forming our
optimization targets:</p>
<h3 id="1-scrape-duration">1. Scrape duration<a hidden class="anchor" aria-hidden="true" href="#1-scrape-duration">#</a></h3>
<p>The reoccurring (e.g. every minute) process of Prometheus requesting Kubernetes
metrics from kube-state-metrics is called a <em>scrape</em>. Prometheus tracks the
start of the process, initializes the http request to kube-state-metrics
<code>/metrics</code>, parses the response and saves the values with the start timestamp in
its time series database. Both if kube-state-metrics respondes within a second
and within a minute, the values are saved with the same timestamp within the
Prometheus database.</p>
<img src="/static/metric-driven-performance-optimization/v1.4_scrape_duration_seconds.png">
<p>Once scrape durations differ within consecutive scrapes of the same exporter
(e.g. kube-state-metrics) or across exporters (kube-state-metrics and <a href="https://github.com/google/cadvisor">cadvisor
</a>), it is difficult to correlate the
collected data, as it could be off by as much as the scraping process lasted
(minus network latency, http, tcp, ip overhead, &hellip;). Thus it is disirable to
keep the scrape duration to a minimum. In addition Prometheus even times out by
default <a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration-file"> once a scrape duration reaches
10s</a>.</p>
<p>The metric <em>scrape_duration_seconds</em> exposed by Prometheus itself gives us
visibility for the above, hence we can use it for the planned optimization.</p>
<h3 id="2-cpu-usage">2. CPU usage<a hidden class="anchor" aria-hidden="true" href="#2-cpu-usage">#</a></h3>
<p>Running kube-state-metrics on large Kubernetes clusters results in high CPU
usage of kube-state-metrics as it has to convert the Kubernetes API objects to
Prometheus metrics ad-hoc on each scrape of Prometheus. In addition this CPU
load is not a flat line, but spiky, given that this CPU intensive process is
part of the hot-path of Prometheus scraping the exporter.</p>
<img src="/static/metric-driven-performance-optimization/v1.4_container_cpu_usage_seconds.png">
<p>As with the <em>scrape_duration_seconds</em> metric for the first optimization target,
we can leverage the fact that kube-state-metrics can run on a Kubernetes cluster
and use the <em>container_cpu_usage_seconds</em> exposed by cadvisor to get visibility
into optimizing cpu usage.</p>
<h3 id="3-memory-usage">3. Memory usage<a hidden class="anchor" aria-hidden="true" href="#3-memory-usage">#</a></h3>
<p>As mentioned above, kube-state-metrics uses the Kubernetes client-go library to
inter-operate with Kubernetes. More specifically it leverages client-go&rsquo;s
<a href="https://godoc.org/k8s.io/client-go/informers">informers</a>. While using informers
keeps kube-state-metric&rsquo;s code complexity low, it forces high memory usage upon
it, given that kube-state-metrics is interested in all Kubernetes objects of a
cluster, thus keeping a copy of all Kubernetes objects within the cache of the
infomers. This is not problematic for small Kubernetes clusters but results in
large memory allocations on larger ones.</p>
<p>Analogous to <em>container_cpu_usage_seconds</em> we can use
<em>container_memory_usage_bytes</em> to track the memory usage of kube-state-metrics.</p>
<h2 id="introducing-a-custom-cache">Introducing a custom cache<a hidden class="anchor" aria-hidden="true" href="#introducing-a-custom-cache">#</a></h2>
<p>As previously mentioned kube-state-metrics v1.4 and below replicates all
Kubernetes objects into its internal cache to ad-hoc convert them into
the Prometheus metric format, once a new scrape request comes in.</p>
<small>
```
+-----------+      +-------------------+                       +-------------+
| APIServer |      | kubestatemetrics  |                       | Prometheus  |
+-----------+      +-------------------+                       +-------------+
      |                      |                                        |
      | New Pod p1           |                                        |
      |--------------------->|                                        |
      |         -----------\ |                                        |
      |         | Cache p1 |-|                                        |
      |         |----------| |                                        |
      |                      |                                        |
      |                      |                                        |
      |                      |                                        |
      |                      |                        Get all metrics |
      |                      |<---------------------------------------|
      |                      | --------------------------------\      |
      |                      |-| Render all Kubernetes objects |      |
      |                      | | into metrics                  |      |
      |                      | |-------------------------------|      |
      |                      |                                        |
      |                      | Metrics                                |
      |                      |--------------------------------------->|
      |                      |                                        |
<pre tabindex="0"><code>&lt;/small&gt;

This lead us to an idea. Instead of caching the Kubernetes objects themselves,
how about rendering them into Prometheus metrics right away and only cache the
rendered metrics instead. This has three advantages: First Kubernetes object to
metric rendering is not happening in the hot-path (scrape) anymore reducing
scrape duration. Second caching metrics instead of full Kubernetes objects
reduces needed memory. Third, under the premise that most Kubernetes objects
stay untouched between consecutive scrapes, metrics are rendered once per
Kubernetes object update, not once per scrape, reducing CPU usage.

&lt;small&gt;
</code></pre><p>+&mdash;&mdash;&mdash;&ndash;+           +&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-+       +&mdash;&mdash;&mdash;&mdash;-+
| APIServer |           | kubestatemetrics  |       | Prometheus  |
+&mdash;&mdash;&mdash;&ndash;+           +&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-+       +&mdash;&mdash;&mdash;&mdash;-+
|                           |                        |
| New Pod p1                |                        |
|&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;&gt;|                        |
|  &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;\ |                        |
|  | Render metrics of p1 |-|                        |
|  |&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-| |                        |
|&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;-\ |                        |
|| Cache rendered metrics |-|                        |
||&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;| |                        |
|                           |                        |
|                           |                        |
|                           |                        |
|                           |        Get all metrics |
|                           |&lt;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;|
|                           |                        |
|                           | Metrics                |
|                           |&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;&gt;|
|                           |                        |</p>
<pre tabindex="0"><code>&lt;/small&gt;

Instead of replacing all of Kubernetes client-go with our own specialized
implementation, which increases the maintenance burdon given that Kubernetes is a
fast moving project, it is possible to hook into client-go via the [ *Reflector*
](https://github.com/kubernetes/client-go/blob/master/tools/cache/reflector.go)
abstraction and implement a custom cache, in our case for metrics instead of
Kubernetes objects.

The result can be seen in the graph below, showing the duration of Prometheus
scrapes of the old version (v1.4) in red and the optimized version in cyan.
Introducing the optimized cache lead to a ~10x improvement in our test
environment. Please keep in mind that this was under artificial load. Numbers on
real world clusters will be shown further below.

&lt;img src=&#34;/static/metric-driven-performance-optimization/result-caching.png&#34;&gt;


## Compression on or off?

Next up we looked into compression of the metric payload going back to
Prometheus. By default kube-state-metrics v1.4, more precisely Prometheus
client_golang, compressed its responses. Doing so sounds reasonable, given that
the Prometheus metric format is repetitive, thus great for compression, but
still we wanted to investigate this default option, given that most deployments
are running in a local area network.

We deployed kube-state-metrics in four versions: v1.4, cache-optimized without
compression, cache-optimized with golang gzip, cache-optimized with [New York
Times gzip](https://github.com/NYTimes/gziphandler). First off we can compare
the CPU usage. Without surprise compressing is more CPU intense:

&lt;img src=&#34;/static/metric-driven-performance-optimization/compression_cpu_usage.png&#34;&gt;

Next we can take a look at the memory usage (not allocation). One would think
that using compression kube-state-metrics would use less memory. Problem is,
that we are not caching a big blob of metrics of all Kubernetes objects, but
instead we are caching multiple blobs of metrics per Kubernetes object. That
way, once an update for a single Kubernetes object comes in, one does not have
to recompute the big blob, but only the metric blob for that specific Kubernetes
object. With this in mind, for a scrape we have to cumulate all the blobs and
then compress. Thus compression does not decrease the memory footprint, but
quite the opposite, increases it.

&lt;img src=&#34;/static/metric-driven-performance-optimization/compression_memory_usage.png&#34;&gt;

Last we should take a look at the impact of compression on the scrape duration.
In our test environment Prometheus and kube-state-metrics are within the same
local area network. The graph below shows, that transfering bigger metric blobs
in plain-text is faster than transfering smaller compressed metric blobs +
compressing and decompressing. Thus, as you can see in the graph below
kube-state-metrics with optimized caching but disabled compression has the
lowest scrape duration.

&lt;img src=&#34;/static/metric-driven-performance-optimization/compression_scrape_duration.png&#34;&gt;

After testing this in multiple real world scenarios, we ended up adding the
option to compress to the new version of kube-state-metrics but disabling it by
default.


## Golangs _strings.Builder_

While optimizing kube-state-metrics itself, a lot happened on the Prometheus
upstream [common library](https://github.com/prometheus/common/pull/148) in
parallel. Among other things the library switched to using Golangs [
strings.Builder ](https://golang.org/pkg/strings/#Builder). Instead of
formatting strings via `fmt`, which bears many memory allocations with it,
`strings.Builder` uses a byte slice under the hood, leveraging the fact that
byte slices in constrary to strings are mutable. Many of these optimizations
also landed downstream in kube-state-metrics.


## Benchmarks

Given the wide spread usage of kube-state-metrics in the Kubernetes community,
many users helped us to test the performance optimized version of kube-state-metrics.
You can find multiple benchmarks in [ this
](https://github.com/kubernetes/kube-state-metrics/issues/498)
Github thread.


---

This blog post is based on a [talk I gave at KubeCon Barcelona
2019](https://kccnceu19.sched.com/event/MPjo/deep-dive-kubernetes-instrumentation-sig-frederic-branczyk-max-inden-red-hat).
The [recording](https://www.youtube.com/watch?v=dvk_-NCK1Ls) is online. The
[slides can be downloaded
here](/static/metric-driven-performance-optimization/slides.pdf).
</code></pre>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://max-inden.de/tags/tech/">Tech</a></li>
      <li><a href="https://max-inden.de/tags/prometheus/">Prometheus</a></li>
      <li><a href="https://max-inden.de/tags/monitoring/">Monitoring</a></li>
      <li><a href="https://max-inden.de/tags/kubernetes/">Kubernetes</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://max-inden.de/">Max Inden</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
